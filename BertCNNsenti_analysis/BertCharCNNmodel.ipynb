{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "#from pytorch_transformers import BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    documents = documents.str.replace(\"[^a-zA-Z0-9 ]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        #tokens = nltk.tokenize.word_tokenize(sentence) # 토큰화\n",
    "        #tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')] # 불용어 제거\n",
    "        #clean_texts = \" \".join(tokens)\n",
    "        clean_texts = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', '', sentence) # 특수문자, 임티 제거\n",
    "        clean_texts = re.sub(' +', ' ', clean_texts) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen):\n",
    "    \"\"\"Right padding.\"\"\"\n",
    "    pad_seq = []\n",
    "    for sequence in sequences:\n",
    "        current_len = len(sequence)\n",
    "        if current_len > maxlen:\n",
    "            pad_seq.append(sequence[: maxlen - 1])\n",
    "        else:\n",
    "            extra = maxlen - current_len\n",
    "            pad_seq.append(sequence + ([0] * extra))\n",
    "\n",
    "    return pad_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(max_len, train_x, test_x):\n",
    "    \n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "    #print(squeezed_data)\n",
    "\n",
    "    #sequences = [tokenizer.batch_encode_plus(\"[CLS] \"+ t +\" [SEP]\", add_special_tokens=True, return_attention_mask=True, padding='longest') for t in squeezed_data] # 문자를 시퀀스로 바꿈\n",
    "    #sequences = tokenizer.batch_encode_plus(squeezed_data, add_special_tokens=True, return_attention_mask=True, padding='longest')\n",
    "    sequences =tokenizer.batch_encode_plus(squeezed_data, add_special_tokens=True, return_attention_mask=True, padding='longest')\n",
    "    #print('시퀀스 길이', len(sequences[0]))\n",
    "    #print('첫번째 시퀀스 예시', sequences[0])\n",
    "\n",
    "    #print('총 단어 수 : ', len(tok.word_index))\n",
    "\n",
    "    train_result = sequences['input_ids'][:div]\n",
    "    test_result = sequences['input_ids'][div:]\n",
    "    \n",
    "    train_atmask = sequences['attention_mask'][:div]\n",
    "    test_atmask = sequences['attention_mask'][div:]\n",
    "\n",
    "    return train_result, test_result, train_atmask, test_atmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv(\"train_final.csv\")\n",
    "    eval = pd.read_csv(\"eval_final_open.csv\")\n",
    "    train_X = train['Sentence'] # 본문 내용을 기준으로 분류하기\n",
    "    train_Y = train['Category']\n",
    "    eval_X = eval['Sentence']\n",
    "    return train_X, train_Y, eval_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터셋 크기 :  16964\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "step1_flag = True\n",
    "if not step1_flag:\n",
    "    print(train_x[:10])\n",
    "\n",
    "    train_x, test_x = preprocessing(train_x), preprocessing(test_x)\n",
    "    pd.DataFrame(train_x).to_csv('temp_train_x.csv')\n",
    "    pd.DataFrame(train_y).to_csv('temp_train_y.csv')\n",
    "    pd.DataFrame(test_x).to_csv('temp_test_x.csv')\n",
    "    #print(train_x[:10])\n",
    "else:\n",
    "    # 임시 불러오기\n",
    "    train_x, test_x = pd.read_csv('temp_train_x.csv'), pd.read_csv('temp_test_x.csv')\n",
    "    #print(train_x)\n",
    "    train_x = train_x.iloc[:,1:].values.tolist()\n",
    "    test_x = test_x.iloc[:,1:].values.tolist()\n",
    "    #print(train_x)\n",
    "    #y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "    max_len = 70\n",
    "    #print(train_x[0])\n",
    "    x_train, x_test, train_atmask, test_atmask = Tokenization(max_len, train_x, test_x)\n",
    "    # print(x_train)\n",
    "    pd.DataFrame(x_train).to_csv('final_train_x.csv')\n",
    "    #pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "    pd.DataFrame(x_test).to_csv('final_test_x.csv')\n",
    "    pd.DataFrame(train_atmask).to_csv('final_train_atmask.csv')\n",
    "    pd.DataFrame(test_atmask).to_csv('final_test_atmask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_char(documents):\n",
    "    documents = documents.str.replace(\"[ãàáâçèéíïñóöôûüæ\\t\\n\\s]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        sentence = sentence.lower()\n",
    "        clean_texts = re.sub(' ', '', sentence) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_char(max_len, train_x, test_x, alphabet =\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    \n",
    "    alpha_len = len(alphabet)\n",
    "    id_mat = np.identity(alpha_len + 12)\n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    #print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "    \n",
    "    max_len = 300\n",
    "    encode_data_set = np.zeros((len(dataset), alpha_len + 12, max_len))\n",
    "    for idx, t in enumerate(squeezed_data):\n",
    "        char_seq = []\n",
    "        for i, c in enumerate(t):\n",
    "            if i >= max_len:\n",
    "                break\n",
    "            if alphabet.find(c) == -1:\n",
    "                print(c)\n",
    "                print(\"error\")\n",
    "                break\n",
    "            char_seq.append(id_mat[alphabet.find(c)].reshape(-1,1))\n",
    "        \n",
    "        char_len = len(char_seq)\n",
    "        if char_len < max_len:\n",
    "            char_seq.append(np.zeros((alpha_len + 12, max_len - char_len)))\n",
    "        \n",
    "        encode_data_set[idx] = np.hstack(char_seq)\n",
    "        if idx == 0:\n",
    "            print(encode_data_set[idx])\n",
    "\n",
    "    \n",
    "\n",
    "    train_result = encode_data_set[:div]\n",
    "    test_result = encode_data_set[div:]\n",
    "\n",
    "    return train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"-lrb-thefilm-rrb-tacklesthetopicofrelationshipsinsuchastraightforward,emotionallyhonestmannerthatbytheend,it'simpossibletoascertainwhetherthefilmis,atitscore,deeplypessimisticorquietlyhopeful.\"]\n",
      "['lavishly,exhilaratinglytasteless.']\n",
      "['itisalsobeautifullyacted.']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "step1_flag = True\n",
    "if not step1_flag:\n",
    "\n",
    "    train_x, test_x = preprocessing_char(train_x), preprocessing_char(test_x)\n",
    "    pd.DataFrame(train_x).to_csv('char_temp_train_x.csv')\n",
    "    pd.DataFrame(train_y).to_csv('char_temp_train_y.csv')\n",
    "    pd.DataFrame(test_x).to_csv('char_temp_test_x.csv')\n",
    "    #print(train_x[:10])\n",
    "else:\n",
    "    # 임시 불러오기\n",
    "    train_x, test_x = pd.read_csv('char_temp_train_x.csv'), pd.read_csv('char_temp_test_x.csv')\n",
    "    #print(train_x)\n",
    "    train_x = train_x.iloc[:,1:].values.tolist()\n",
    "    test_x = test_x.iloc[:,1:].values.tolist()\n",
    "    print(train_x[0])\n",
    "    print(train_x[1])\n",
    "    print(train_x[2])\n",
    "    #y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "    max_len = 70\n",
    "    #print(train_x[0])\n",
    "    x_train, x_test = Tokenization_char(max_len, train_x, test_x)\n",
    "    print(x_train)\n",
    "    pickle.dump(x_train, open(\"./char_final_train_x.pkl\",\"wb\"))\n",
    "    #pd.DataFrame(x_train).to_csv('char_final_train_x.csv')\n",
    "    #pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "    pickle.dump(x_test, open(\"./char_final_test_x.pkl\",\"wb\"))\n",
    "    #pd.DataFrame(x_test).to_csv('char_final_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mxnet.gluon import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "#from pytorch_transformers import BertConfig, BertModel\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, number_of_characters, max_len, dropout=0.1, number_of_classes=5, batch_size = 32):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "\n",
    "        # define conv layers\n",
    "\n",
    "        self.dropout_input = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(number_of_characters ,256,kernel_size=7, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "\n",
    "        input_shape = (batch_size, max_len, number_of_characters)\n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        # define linear layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.output_dimension, 1024), nn.ReLU(), nn.Dropout(0.1))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(0.1))\n",
    "        self.fc3 = nn.Linear(1024, number_of_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self._create_weights()\n",
    "\n",
    "    # utility private functions\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        \n",
    "        return output_dimension\n",
    "\n",
    "    # forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.dropout_input(x)\n",
    "        #print(x.size())\n",
    "        #x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = F.softmax(x)\n",
    "        #print(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertCNNClassifier(nn.Module):\n",
    "    def __init__(self, bert, charcnn, num_classes=5, num_classifier=1, dropout=None):\n",
    "        super(MyBertCNNClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.charcnn =charcnn\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classifier = num_classifier\n",
    "        \n",
    "        self.classifiers = nn.Sequential(\n",
    "        nn.BatchNorm1d(768 + 1024),\n",
    "        nn.Linear(768 + 1024, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(128, num_classes),\n",
    "        #nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        #self.classifiers = torch.nn.ModuleList(self.classifiers)\n",
    "        self.classifiers_parameters = self.classifiers.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        #print(bert_out[1].size(), charcnn_out.size())\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        pred = self.classifiers(cat_in)\n",
    "        #pred = torch.mean(torch.cat(cls_pred, dim = 0), dim = 0)\n",
    "        return pred\n",
    "    '''\n",
    "    def predict(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        cls_pred = torch.hstack([torch.argmax(cls(cat_in), dim = 1).view(-1,1) for cls in self.classifiers])\n",
    "        pred, _ = torch.mode(cls_pred)\n",
    "        \n",
    "        return pred\n",
    "       ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertCNNClassifier2(nn.Module):\n",
    "    def __init__(self, bert, charcnn, num_classes=5, num_classifier=1, dropout=None):\n",
    "        super(MyBertCNNClassifier2, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.charcnn =charcnn\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classifier = num_classifier\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(768, num_classes))\n",
    "        \n",
    "        self.classifier_char = nn.Sequential(\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(128, num_classes))\n",
    "\n",
    "        \n",
    "        self.classifiers = torch.nn.ModuleList([self.classifier, self.classifier_char])\n",
    "        self.classifiers_parameters = self.classifiers.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        #charcnn_out = self.charcnn(_x)\n",
    "        pred_bert = self.classifier(bert_out[1])\n",
    "        #pred_char = self.classifier_char(charcnn_out)\n",
    "        pred = pred_bert\n",
    "        \n",
    "        #print(bert_out[1].size(), charcnn_out.size())\n",
    "        #cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        #pred = [cls(cat_in) for cls in self.classifiers]\n",
    "        #pred = torch.mean(torch.cat(cls_pred, dim = 0), dim = 0)\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        cls_pred = torch.hstack([torch.argmax(cls(cat_in), dim = 1).view(-1,1) for cls in self.classifiers])\n",
    "        pred, _ = torch.mode(cls_pred)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, N, lossfn, optimizer,scheduler, X, atmask, _X, Y, batch_size=16):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    #print(Y)\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_train_x, batch_atmask, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * nb:]).long().to(device),torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "            #batch_train_x, batch_train_y = torch.tensor(X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_train_x, batch_atmask, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device),torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_train_x, batch_atmask, batch_train_char_x)\n",
    "        loss = lossfn(logits, batch_train_y)\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        model.eval()\n",
    "        pred_labels = torch.argmax(logits, dim = 1)\n",
    "        #print(pred_labels)\n",
    "        train_acc += (pred_labels == batch_train_y).sum().item()\n",
    "        \n",
    "        del batch_train_x\n",
    "        del batch_atmask\n",
    "        del batch_train_char_x\n",
    "        del batch_train_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    train_loss /= N\n",
    "    train_acc /= N\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, N, lossfn, X, atmask, _X,  Y, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    loss, acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_valid_x, batch_atmask, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * nb:]).long().to(device),torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_valid_x, batch_atmask, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device),torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "        \n",
    "        logits = model(batch_valid_x, batch_atmask, batch_valid_char_x)\n",
    "\n",
    "        loss += lossfn(logits, batch_valid_y).item() \n",
    "            \n",
    "        #pred_labels = model.predict(batch_valid_x, batch_valid_char_x)\n",
    "        pred_labels = torch.argmax(logits, dim = 1)\n",
    "        acc += (pred_labels == batch_valid_y).sum().item()\n",
    "            \n",
    "        del batch_valid_x\n",
    "        del batch_valid_char_x\n",
    "        del batch_valid_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss /= N\n",
    "    acc /= N\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, N, lossfn, X, atmask, _X, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    test_y = None\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_test_x, batch_atmask, batch_test_char_x = torch.tensor(X[batch_size * nb:]).long().to(device), torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device)\n",
    "        else:\n",
    "            batch_test_x, batch_atmask, batch_test_char_x = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device)\n",
    "\n",
    "        logits = model(batch_test_x, batch_atmask, batch_test_char_x)\n",
    "        batch_test_y = torch.argmax(logits, dim = 1).detach().cpu().numpy()\n",
    "        if i == 0:\n",
    "            test_y = batch_test_y\n",
    "        else:\n",
    "            test_y = np.concatenate([test_y, batch_test_y])\n",
    "            \n",
    "        del batch_test_x\n",
    "        del batch_atmask\n",
    "        del batch_test_char_x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(binary=False, bert=\"bert-base-uncased\", epochs=100, batch_size=8, train_prop = 0.98, alphabet = \"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    #\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\"\n",
    "    #\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"\n",
    "    data_x, test_x, data_y = np.array(pd.read_csv('final_train_x.csv').iloc[:,1:]), np.array(pd.read_csv('final_test_x.csv').iloc[:,1:]), np.array(pd.read_csv('temp_train_y.csv').iloc[:,1:])\n",
    "    data_atmask, test_atmask = np.array(pd.read_csv('final_train_atmask.csv').iloc[:,1:]), np.array(pd.read_csv('final_test_atmask.csv').iloc[:,1:])\n",
    "    data_char_x, test_char_x = pickle.load(open('char_final_train_x.pkl','rb')), pickle.load(open('char_final_test_x.pkl','rb'))\n",
    "    #print(data_x)\n",
    "    data_len = data_x.shape[0]\n",
    "    test_len = test_x.shape[0]\n",
    "    train_div = np.int32(data_len * train_prop)\n",
    "    comp_data = list(zip(data_x, data_char_x, data_atmask, data_y))\n",
    "    np.random.shuffle(comp_data)\n",
    "    data_x, data_char_x, data_atmask, data_y = zip(*comp_data)\n",
    "    data_x, data_char_x, data_atmask, data_y = np.array(data_x), np.array(data_char_x), np.array(data_atmask), np.array(data_y)\n",
    "    train_x, train_atmask, train_char_x, train_y = data_x[:train_div], data_atmask[:train_div], data_char_x[:train_div], data_y[:train_div].reshape(-1)\n",
    "    valid_x, valid_atmask, valid_char_x, valid_y = data_x[train_div:], data_atmask[train_div:], data_char_x[train_div:], data_y[train_div:].reshape(-1)\n",
    "\n",
    "\n",
    "    config = BertConfig.from_pretrained(bert)\n",
    "    if not binary:\n",
    "        config.num_labels = 5\n",
    "    bert_model = BertModel.from_pretrained(bert, config=config)\n",
    "    charcnn_model = CharacterLevelCNN(number_of_characters = len(alphabet) + 12, max_len = 300)\n",
    "    #charcnn_model.load_state_dict(copy.deepcopy(torch.load(\"char_cnn_pre.pth\", torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))))\n",
    "    \n",
    "    model = MyBertCNNClassifier(bert_model, charcnn_model, num_classes=5, dropout=0.1)\n",
    "    model.load_state_dict(torch.load(\"./pretrained_BERTCNN15.pth\"))\n",
    "\n",
    "    #temp_charcnn_model = torch.load(\"char_cnn_pre.pth\", torch.device(\"cpu\"))\n",
    "    #pretrained_dict = load_state_dict(torch.load(\"./pretrained_BERTCNN15.pth\"))\n",
    "    #model_dict = model.state_dict()\n",
    "\n",
    "    # 1. filter out unnecessary keys\n",
    "    #temp_dict = {}\n",
    "    #for _k, k in zip(pretrained_dict.keys(), model_dict.keys()):\n",
    "    #    model_dict[k] = pretrained_dict[_k]\n",
    "    #{k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    #pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    #print(temp_dict)\n",
    "    #model_dict.update(temp_dict) \n",
    "    # 3. load the new state dict\n",
    "    #charcnn_model.load_state_dict(model_dict)\n",
    "\n",
    "    #model = MyBertCNNClassifier(bert_model, charcnn_model, num_classes=5, dropout=0.1)\n",
    "    #model.load_state_dict(torch.load(\"./pretrained_BERTCNN15.pth\"))\n",
    "\n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps= (train_div // batch_size + 1)  * epochs)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_div, lossfn, optimizer, scheduler, train_x , train_atmask, train_char_x, train_y, batch_size=batch_size)\n",
    "        val_loss, val_acc = evaluate_one_epoch(model, data_len - train_div, lossfn,  valid_x, valid_atmask, valid_char_x, valid_y, batch_size=batch_size)\n",
    "        #scheduler.step(val_loss)\n",
    "        test_out = test_one_epoch(model, test_len, lossfn, test_x,test_atmask, test_char_x, batch_size=batch_size)\n",
    "        sub_data = pd.DataFrame(test_out.reshape((-1,1)), columns=['Category'])\n",
    "        #test_loss, test_acc = evaluate_one_epoch(model, lossfn, optimizer, testset, batch_size=batch_size)\n",
    "\n",
    "        print(f\"epoch={epoch}\")\n",
    "        print(f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        print(f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "        sub_data.to_csv('submission'+str(epoch)+'.csv', index_label = ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1\n",
      "train_loss=0.0187, val_loss=0.0129\n",
      "train_acc=0.973, val_acc=0.976\n",
      "epoch=2\n",
      "train_loss=0.0179, val_loss=0.0077\n",
      "train_acc=0.972, val_acc=0.976\n",
      "epoch=3\n",
      "train_loss=0.0129, val_loss=0.0054\n",
      "train_acc=0.979, val_acc=0.988\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
