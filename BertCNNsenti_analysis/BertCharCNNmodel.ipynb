{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "#from pytorch_transformers import BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    documents = documents.str.replace(\"[^a-zA-Z0-9 ]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        #tokens = nltk.tokenize.word_tokenize(sentence) # 토큰화\n",
    "        #tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')] # 불용어 제거\n",
    "        #clean_texts = \" \".join(tokens)\n",
    "        clean_texts = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', '', sentence) # 특수문자, 임티 제거\n",
    "        clean_texts = re.sub(' +', ' ', clean_texts) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen):\n",
    "    \"\"\"Right padding.\"\"\"\n",
    "    pad_seq = []\n",
    "    for sequence in sequences:\n",
    "        current_len = len(sequence)\n",
    "        if current_len > maxlen:\n",
    "            pad_seq.append(sequence[: maxlen - 1])\n",
    "        else:\n",
    "            extra = maxlen - current_len\n",
    "            pad_seq.append(sequence + ([0] * extra))\n",
    "\n",
    "    return pad_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(max_len, train_x, test_x):\n",
    "    \n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "    #print(squeezed_data)\n",
    "\n",
    "    #sequences = [tokenizer.batch_encode_plus(\"[CLS] \"+ t +\" [SEP]\", add_special_tokens=True, return_attention_mask=True, padding='longest') for t in squeezed_data] # 문자를 시퀀스로 바꿈\n",
    "    #sequences = tokenizer.batch_encode_plus(squeezed_data, add_special_tokens=True, return_attention_mask=True, padding='longest')\n",
    "    sequences =tokenizer.batch_encode_plus(squeezed_data, add_special_tokens=True, return_attention_mask=True, padding='longest')\n",
    "    #print('시퀀스 길이', len(sequences[0]))\n",
    "    #print('첫번째 시퀀스 예시', sequences[0])\n",
    "\n",
    "    #print('총 단어 수 : ', len(tok.word_index))\n",
    "\n",
    "    train_result = sequences['input_ids'][:div]\n",
    "    test_result = sequences['input_ids'][div:]\n",
    "    \n",
    "    train_atmask = sequences['attention_mask'][:div]\n",
    "    test_atmask = sequences['attention_mask'][div:]\n",
    "\n",
    "    return train_result, test_result, train_atmask, test_atmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    _train = pd.read_csv(\"./train.txt\", sep=\"\\t\", names=['Category', 'Sentence'], header=None)\n",
    "    #___train = pd.read_csv(\"./train_x_de2en.txt\", sep=\"\\t\", names=['Category', 'Sentence'], header=None)\n",
    "\n",
    "    __train = pd.read_csv(\"train_final.csv\")\n",
    "        \n",
    "    #train = pd.concat([__train, _train])\n",
    "    train = __train\n",
    "    #train.dropna(axis = 0, how = 'any', inplace = True)\n",
    "    eval = pd.read_csv(\"eval_final_open.csv\")\n",
    "    \n",
    "    \n",
    "    train_X = train['Sentence'] # 본문 내용을 기준으로 분류하기\n",
    "    train_Y = train['Category']\n",
    "    eval_X = eval['Sentence']\n",
    "    return train_X, train_Y, eval_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    -LRB- The film -RRB- tackles the topic of rela...\n",
      "1                Lavishly , exhilaratingly tasteless .\n",
      "2                       It is also beautifully acted .\n",
      "3    But , like Silence , it 's a movie that gets u...\n",
      "4    It 's been made with an innocent yet fervid co...\n",
      "5    Director David Jacobson gives Dahmer a conside...\n",
      "6    Overall , Cletis Tout is a winning comedy that...\n",
      "7    The movie is too impressed with its own solemn...\n",
      "8    `` -LRB- Hopkins -RRB- does n't so much phone ...\n",
      "9                                      Delirious fun .\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "\n",
    "print(train_x[:10])\n",
    "\n",
    "train_x, test_x = preprocessing(train_x), preprocessing(test_x)\n",
    "pd.DataFrame(train_x).to_csv('temp_train_x.csv')\n",
    "pd.DataFrame(train_y).to_csv('temp_train_y.csv')\n",
    "pd.DataFrame(test_x).to_csv('temp_test_x.csv')\n",
    "#print(train_x[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터셋 크기 :  15855\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x = pd.read_csv('temp_train_x.csv'), pd.read_csv('temp_test_x.csv')\n",
    "#print(train_x)\n",
    "train_x = train_x.iloc[:,1:].values.tolist()\n",
    "test_x = test_x.iloc[:,1:].values.tolist()\n",
    "#print(train_x)\n",
    "#y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "max_len = 70\n",
    "#print(train_x[0])\n",
    "x_train, x_test, train_atmask, test_atmask = Tokenization(max_len, train_x, test_x)\n",
    "# print(x_train)\n",
    "pd.DataFrame(x_train).to_csv('final_train_x.csv')\n",
    "#pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "pd.DataFrame(x_test).to_csv('final_test_x.csv')\n",
    "pd.DataFrame(train_atmask).to_csv('final_train_atmask.csv')\n",
    "pd.DataFrame(test_atmask).to_csv('final_test_atmask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_char(documents):\n",
    "    documents = documents.str.replace(\"[ãàáâçèéíïñóöôûüæ\\t\\n\\s]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        sentence = sentence.lower()\n",
    "        clean_texts = re.sub(' ', '', sentence) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_char(max_len, train_x, test_x, alphabet =\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    \n",
    "    alpha_len = len(alphabet)\n",
    "    id_mat = np.identity(alpha_len + 12)\n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    #print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "    \n",
    "    max_len = 300\n",
    "    encode_data_set = np.zeros((len(dataset), alpha_len + 12, max_len))\n",
    "    for idx, t in enumerate(squeezed_data):\n",
    "        char_seq = []\n",
    "        for i, c in enumerate(t):\n",
    "            if i >= max_len:\n",
    "                break\n",
    "            if alphabet.find(c) == -1:\n",
    "                print(c)\n",
    "                print(\"error\")\n",
    "                break\n",
    "            char_seq.append(id_mat[alphabet.find(c)].reshape(-1,1))\n",
    "        \n",
    "        char_len = len(char_seq)\n",
    "        if char_len < max_len:\n",
    "            char_seq.append(np.zeros((alpha_len + 12, max_len - char_len)))\n",
    "        \n",
    "        encode_data_set[idx] = np.hstack(char_seq)\n",
    "        if idx == 0:\n",
    "            print(encode_data_set[idx])\n",
    "\n",
    "    \n",
    "\n",
    "    train_result = encode_data_set[:div]\n",
    "    test_result = encode_data_set[div:]\n",
    "\n",
    "    return train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "\n",
    "train_x, test_x = preprocessing_char(train_x), preprocessing_char(test_x)\n",
    "pd.DataFrame(train_x).to_csv('char_temp_train_x.csv')\n",
    "pd.DataFrame(train_y).to_csv('char_temp_train_y.csv')\n",
    "pd.DataFrame(test_x).to_csv('char_temp_test_x.csv')\n",
    "#print(train_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"-lrb-thefilm-rrb-tacklesthetopicofrelationshipsinsuchastraightforward,emotionallyhonestmannerthatbytheend,it'simpossibletoascertainwhetherthefilmis,atitscore,deeplypessimisticorquietlyhopeful.\"]\n",
      "['lavishly,exhilaratinglytasteless.']\n",
      "['itisalsobeautifullyacted.']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 임시 불러오기\n",
    "train_x, test_x = pd.read_csv('char_temp_train_x.csv'), pd.read_csv('char_temp_test_x.csv')\n",
    "#print(train_x)\n",
    "train_x = train_x.iloc[:,1:].values.tolist()\n",
    "test_x = test_x.iloc[:,1:].values.tolist()\n",
    "print(train_x[0])\n",
    "print(train_x[1])\n",
    "print(train_x[2])\n",
    "#y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "max_len = 70\n",
    "#print(train_x[0])\n",
    "x_train, x_test = Tokenization_char(max_len, train_x, test_x)\n",
    "print(x_train)\n",
    "pickle.dump(x_train, open(\"./char_final_train_x.pkl\",\"wb\"))\n",
    "#pd.DataFrame(x_train).to_csv('char_final_train_x.csv')\n",
    "#pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "pickle.dump(x_test, open(\"./char_final_test_x.pkl\",\"wb\"))\n",
    "#pd.DataFrame(x_test).to_csv('char_final_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mxnet.gluon import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "#from pytorch_transformers import BertConfig, BertModel\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, number_of_characters, max_len, dropout=0.1, number_of_classes=5, batch_size = 32):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "\n",
    "        # define conv layers\n",
    "\n",
    "        self.dropout_input = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(number_of_characters ,256,kernel_size=7, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "\n",
    "        input_shape = (batch_size, max_len, number_of_characters)\n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        # define linear layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.output_dimension, 1024), nn.ReLU(), nn.Dropout(0.1))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(0.1))\n",
    "        self.fc3 = nn.Linear(1024, number_of_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self._create_weights()\n",
    "\n",
    "    # utility private functions\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        \n",
    "        return output_dimension\n",
    "\n",
    "    # forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.dropout_input(x)\n",
    "        #print(x.size())\n",
    "        #x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = F.softmax(x)\n",
    "        #print(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertCNNClassifier(nn.Module):\n",
    "    def __init__(self, bert, charcnn, num_classes=5, num_classifier=10, dropout=None):\n",
    "        super(MyBertCNNClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.charcnn =charcnn\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classifier = num_classifier\n",
    "        \n",
    "        self.classifiers = [nn.Sequential(\n",
    "        nn.BatchNorm1d(768 + 1024),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(768 + 1024, num_classes),\n",
    "        #nn.Softmax()\n",
    "        ) for _ in range(self.num_classifier)]\n",
    "        \n",
    "        self.classifiers = torch.nn.ModuleList(self.classifiers)\n",
    "        self.classifiers_parameters = self.classifiers.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        #print(bert_out[1].size(), charcnn_out.size())\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        cls_pred = [cls(cat_in) for cls in self.classifiers]\n",
    "        #pred = self.classifiers(cat_in)\n",
    "        #pred = torch.mean(torch.cat(cls_pred, dim = 0), dim = 0)\n",
    "        return cls_pred\n",
    "\n",
    "    def predict(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        cls_pred = torch.hstack([torch.argmax(cls(cat_in), dim = 1).view(-1,1) for cls in self.classifiers])\n",
    "        pred, _ = torch.mode(cls_pred)\n",
    "        \n",
    "        return pred\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertCNNClassifier2(nn.Module):\n",
    "    def __init__(self, bert, charcnn, num_classes=5, num_classifier=1, dropout=None):\n",
    "        super(MyBertCNNClassifier2, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.charcnn =charcnn\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classifier = num_classifier\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(768, num_classes))\n",
    "        \n",
    "        self.classifier_char = nn.Sequential(\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(1024, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(128, num_classes))\n",
    "\n",
    "        \n",
    "        self.classifiers = torch.nn.ModuleList([self.classifier, self.classifier_char])\n",
    "        self.classifiers_parameters = self.classifiers.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        #charcnn_out = self.charcnn(_x)\n",
    "        pred_bert = self.classifier(bert_out[1])\n",
    "        #pred_char = self.classifier_char(charcnn_out)\n",
    "        pred = pred_bert\n",
    "        \n",
    "        #print(bert_out[1].size(), charcnn_out.size())\n",
    "        #cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        #pred = [cls(cat_in) for cls in self.classifiers]\n",
    "        #pred = torch.mean(torch.cat(cls_pred, dim = 0), dim = 0)\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, x, atmask, _x):\n",
    "        bert_out = self.bert(x, attention_mask = atmask)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        cls_pred = torch.hstack([torch.argmax(cls(cat_in), dim = 1).view(-1,1) for cls in self.classifiers])\n",
    "        pred, _ = torch.mode(cls_pred)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, N, lossfn, optimizer,scheduler, X, atmask, _X, Y, batch_size=16):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    #print(Y)\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_train_x, batch_atmask, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * nb:]).long().to(device),torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "            #batch_train_x, batch_train_y = torch.tensor(X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_train_x, batch_atmask, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device),torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_train_x, batch_atmask, batch_train_char_x)\n",
    "        #print(logits)\n",
    "        #for idx, logit in enumerate(logits):\n",
    "        #    loss = lossfn(logit, batch_train_y)\n",
    "        \n",
    "        flag = False\n",
    "        #sample_idx = np.random.choice(model.num_classifier, size = 5)\n",
    "        for idx, logit in enumerate(logits):\n",
    "            if idx != i % model.num_classifier:\n",
    "                if flag:\n",
    "                    loss += lossfn(logit, batch_train_y)\n",
    "                else:\n",
    "                    loss = lossfn(logit, batch_train_y)\n",
    "                    flag = True\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        model.eval()\n",
    "        pred_labels = model.predict(batch_train_x, batch_atmask, batch_train_char_x)\n",
    "        #print(pred_labels)\n",
    "        train_acc += (pred_labels == batch_train_y).sum().item()\n",
    "        \n",
    "        del batch_train_x\n",
    "        del batch_atmask\n",
    "        del batch_train_char_x\n",
    "        del batch_train_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    train_loss /= N\n",
    "    train_acc /= N\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, N, lossfn, X, atmask, _X,  Y, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    loss, acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_valid_x, batch_atmask, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * nb:]).long().to(device),torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_valid_x, batch_atmask, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device),torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "        \n",
    "        logits = model(batch_valid_x, batch_atmask, batch_valid_char_x)\n",
    "        \n",
    "        for logit in logits:\n",
    "            loss += lossfn(logit, batch_valid_y).item() \n",
    "            \n",
    "        #pred_labels = model.predict(batch_valid_x, batch_valid_char_x)\n",
    "        pred_labels = model.predict(batch_valid_x,batch_atmask, batch_valid_char_x)\n",
    "        acc += (pred_labels == batch_valid_y).sum().item()\n",
    "            \n",
    "        del batch_valid_x\n",
    "        del batch_valid_char_x\n",
    "        del batch_valid_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss /= N\n",
    "    acc /= N\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, N, lossfn, X, atmask, _X, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    test_y = None\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_test_x, batch_atmask, batch_test_char_x = torch.tensor(X[batch_size * nb:]).long().to(device), torch.tensor(atmask[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device)\n",
    "        else:\n",
    "            batch_test_x, batch_atmask, batch_test_char_x = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(atmask[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device)\n",
    "\n",
    "        #logits = model(batch_test_x, batch_atmask, batch_test_char_x)\n",
    "        batch_test_y = model.predict(batch_test_x, batch_atmask, batch_test_char_x).detach().cpu().numpy()\n",
    "        if i == 0:\n",
    "            test_y = batch_test_y\n",
    "        else:\n",
    "            test_y = np.concatenate([test_y, batch_test_y])\n",
    "            \n",
    "        del batch_test_x\n",
    "        del batch_atmask\n",
    "        del batch_test_char_x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(binary=False, bert=\"bert-base-uncased\", epochs=60, batch_size = 8, train_prop = 0.95, alphabet = \"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    #\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\"\n",
    "    #\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"\n",
    "    data_x, test_x, data_y = np.array(pd.read_csv('final_train_x.csv').iloc[:,1:]), np.array(pd.read_csv('final_test_x.csv').iloc[:,1:]), np.array(pd.read_csv('temp_train_y.csv').iloc[:,1:])\n",
    "    data_atmask, test_atmask = np.array(pd.read_csv('final_train_atmask.csv').iloc[:,1:]), np.array(pd.read_csv('final_test_atmask.csv').iloc[:,1:])\n",
    "    data_char_x, test_char_x = pickle.load(open('char_final_train_x.pkl','rb')), pickle.load(open('char_final_test_x.pkl','rb'))\n",
    "    #print(data_x)\n",
    "    data_len = data_x.shape[0]\n",
    "    test_len = test_x.shape[0]\n",
    "    train_div = np.int32(data_len * train_prop)\n",
    "    comp_data = list(zip(data_x, data_char_x, data_atmask, data_y))\n",
    "    np.random.shuffle(comp_data)\n",
    "    data_x, data_char_x, data_atmask, data_y = zip(*comp_data)\n",
    "    data_x, data_char_x, data_atmask, data_y = np.array(data_x), np.array(data_char_x), np.array(data_atmask), np.array(data_y)\n",
    "    train_x, train_atmask, train_char_x, train_y = data_x[:train_div], data_atmask[:train_div], data_char_x[:train_div], data_y[:train_div].reshape(-1)\n",
    "    valid_x, valid_atmask, valid_char_x, valid_y = data_x[train_div:], data_atmask[train_div:], data_char_x[train_div:], data_y[train_div:].reshape(-1)\n",
    "\n",
    "    config = BertConfig.from_pretrained(bert)\n",
    "    if not binary:\n",
    "        config.num_labels = 5\n",
    "    bert_model = BertModel.from_pretrained(bert, config=config)\n",
    "    charcnn_model = CharacterLevelCNN(number_of_characters = len(alphabet) + 12, max_len = 300)\n",
    "    #charcnn_model.load_state_dict(copy.deepcopy(torch.load(\"char_cnn_pre.pth\", torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))))\n",
    "    \n",
    "    model = MyBertCNNClassifier(bert_model, charcnn_model, num_classes=5, dropout=0.1)\n",
    "    #model.load_state_dict(torch.load(\"./pretrained_BERTCNN15.pth\"))\n",
    "    '''\n",
    "    #temp_charcnn_model = torch.load(\"char_cnn_pre.pth\", torch.device(\"cpu\"))\n",
    "    pretrained_dict = torch.load(\"./pretrained_BERTCNN15.pth\")\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # 1. filter out unnecessary keys\n",
    "    #temp_dict = {}\n",
    "    for _k, k in zip(pretrained_dict.keys(), model_dict.keys()):\n",
    "        model_dict[k] = pretrained_dict[_k]\n",
    "    #{k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    #pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    #print(temp_dict)\n",
    "    #model_dict.update(temp_dict) \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    #model = MyBertCNNClassifier(bert_model, charcnn_model, num_classes=5, dropout=0.1)\n",
    "    #model.load_state_dict(torch.load(\"./pretrained_BERTCNN15.pth\"))'''\n",
    "    \n",
    "    temp_charcnn_model = torch.load(\"char_cnn_pre.pth\", torch.device(\"cpu\"))\n",
    "    #print(temp_charcnn_model.state_dict())\n",
    "    pretrained_dict = temp_charcnn_model.state_dict()\n",
    "    model_dict = charcnn_model.state_dict()\n",
    "\n",
    "    # 1. filter out unnecessary keys\n",
    "    temp_dict = {}\n",
    "    for _k, k in zip(pretrained_dict.keys(), model_dict.keys()):\n",
    "        if \"fc3\" in k:\n",
    "            continue\n",
    "        \n",
    "        model_dict[k] = pretrained_dict[_k]\n",
    "    #{k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    #pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    #print(temp_dict)\n",
    "    #model_dict.update(temp_dict) \n",
    "    # 3. load the new state dict\n",
    "    charcnn_model.load_state_dict(model_dict)\n",
    "\n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps= (train_div // batch_size + 1)  * epochs)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_div, lossfn, optimizer, scheduler, train_x , train_atmask, train_char_x, train_y, batch_size=batch_size)\n",
    "        val_loss, val_acc = evaluate_one_epoch(model, data_len - train_div, lossfn,  valid_x, valid_atmask, valid_char_x, valid_y, batch_size=batch_size)\n",
    "        #scheduler.step(val_loss)\n",
    "        test_out = test_one_epoch(model, test_len, lossfn, test_x,test_atmask, test_char_x, batch_size=batch_size)\n",
    "        sub_data = pd.DataFrame(test_out.reshape((-1,1)), columns=['Category'])\n",
    "        #test_loss, test_acc = evaluate_one_epoch(model, lossfn, optimizer, testset, batch_size=batch_size)\n",
    "\n",
    "        print(f\"epoch={epoch}\")\n",
    "        print(f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        print(f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "        sub_data.to_csv('_submission'+str(epoch)+'.csv', index_label = ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'src.cnn_model.CharacterLevelCNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1\n",
      "train_loss=1.3698, val_loss=1.2798\n",
      "train_acc=0.520, val_acc=0.581\n",
      "epoch=2\n",
      "train_loss=0.9182, val_loss=1.1124\n",
      "train_acc=0.753, val_acc=0.638\n",
      "epoch=3\n",
      "train_loss=0.5872, val_loss=1.4029\n",
      "train_acc=0.886, val_acc=0.644\n",
      "epoch=4\n",
      "train_loss=0.4311, val_loss=1.5982\n",
      "train_acc=0.934, val_acc=0.706\n",
      "epoch=5\n",
      "train_loss=0.3577, val_loss=2.1591\n",
      "train_acc=0.958, val_acc=0.696\n",
      "epoch=6\n",
      "train_loss=0.3122, val_loss=2.2203\n",
      "train_acc=0.970, val_acc=0.734\n",
      "epoch=7\n",
      "train_loss=0.2640, val_loss=2.5005\n",
      "train_acc=0.976, val_acc=0.701\n",
      "epoch=8\n",
      "train_loss=0.2106, val_loss=2.3121\n",
      "train_acc=0.984, val_acc=0.747\n",
      "epoch=9\n",
      "train_loss=0.1500, val_loss=2.4707\n",
      "train_acc=0.990, val_acc=0.747\n",
      "epoch=10\n",
      "train_loss=0.1193, val_loss=2.9489\n",
      "train_acc=0.992, val_acc=0.730\n",
      "epoch=11\n",
      "train_loss=0.1143, val_loss=2.7963\n",
      "train_acc=0.993, val_acc=0.749\n",
      "epoch=12\n",
      "train_loss=0.0726, val_loss=2.9320\n",
      "train_acc=0.995, val_acc=0.747\n",
      "epoch=13\n",
      "train_loss=0.0532, val_loss=3.0391\n",
      "train_acc=0.997, val_acc=0.737\n",
      "epoch=14\n",
      "train_loss=0.0416, val_loss=3.1667\n",
      "train_acc=0.998, val_acc=0.756\n",
      "epoch=15\n",
      "train_loss=0.0284, val_loss=3.4594\n",
      "train_acc=0.999, val_acc=0.728\n",
      "epoch=16\n",
      "train_loss=0.0268, val_loss=3.5105\n",
      "train_acc=0.999, val_acc=0.754\n",
      "epoch=17\n",
      "train_loss=0.0308, val_loss=4.0999\n",
      "train_acc=0.999, val_acc=0.749\n",
      "epoch=18\n",
      "train_loss=0.0266, val_loss=3.5287\n",
      "train_acc=0.999, val_acc=0.751\n",
      "epoch=19\n",
      "train_loss=0.0187, val_loss=3.7690\n",
      "train_acc=0.999, val_acc=0.744\n",
      "epoch=20\n",
      "train_loss=0.0180, val_loss=3.6101\n",
      "train_acc=0.999, val_acc=0.734\n",
      "epoch=21\n",
      "train_loss=0.0106, val_loss=3.5652\n",
      "train_acc=1.000, val_acc=0.746\n",
      "epoch=22\n",
      "train_loss=0.0091, val_loss=3.5211\n",
      "train_acc=1.000, val_acc=0.744\n",
      "epoch=23\n",
      "train_loss=0.0067, val_loss=3.8129\n",
      "train_acc=1.000, val_acc=0.740\n",
      "epoch=24\n",
      "train_loss=0.0088, val_loss=3.8701\n",
      "train_acc=1.000, val_acc=0.728\n",
      "epoch=25\n",
      "train_loss=0.0086, val_loss=4.0067\n",
      "train_acc=1.000, val_acc=0.739\n",
      "epoch=26\n",
      "train_loss=0.0080, val_loss=3.9991\n",
      "train_acc=1.000, val_acc=0.744\n",
      "epoch=27\n",
      "train_loss=0.0037, val_loss=4.2729\n",
      "train_acc=1.000, val_acc=0.737\n",
      "epoch=28\n",
      "train_loss=0.0066, val_loss=4.7341\n",
      "train_acc=1.000, val_acc=0.742\n",
      "epoch=29\n",
      "train_loss=0.0038, val_loss=4.4698\n",
      "train_acc=1.000, val_acc=0.734\n",
      "epoch=30\n",
      "train_loss=0.0041, val_loss=4.4610\n",
      "train_acc=1.000, val_acc=0.753\n",
      "epoch=31\n",
      "train_loss=0.0002, val_loss=4.2896\n",
      "train_acc=1.000, val_acc=0.744\n",
      "epoch=32\n",
      "train_loss=0.0040, val_loss=4.5433\n",
      "train_acc=1.000, val_acc=0.747\n",
      "epoch=33\n",
      "train_loss=0.0038, val_loss=4.6655\n",
      "train_acc=1.000, val_acc=0.756\n",
      "epoch=34\n",
      "train_loss=0.0050, val_loss=4.7247\n",
      "train_acc=1.000, val_acc=0.749\n",
      "epoch=35\n",
      "train_loss=0.0011, val_loss=4.2521\n",
      "train_acc=1.000, val_acc=0.754\n",
      "epoch=36\n",
      "train_loss=0.0015, val_loss=4.3732\n",
      "train_acc=1.000, val_acc=0.770\n",
      "epoch=37\n",
      "train_loss=0.0036, val_loss=4.7065\n",
      "train_acc=1.000, val_acc=0.756\n",
      "epoch=38\n",
      "train_loss=0.0006, val_loss=4.4198\n",
      "train_acc=1.000, val_acc=0.740\n",
      "epoch=39\n",
      "train_loss=0.0034, val_loss=4.4426\n",
      "train_acc=1.000, val_acc=0.753\n",
      "epoch=40\n",
      "train_loss=0.0009, val_loss=4.5206\n",
      "train_acc=1.000, val_acc=0.747\n",
      "epoch=41\n",
      "train_loss=0.0000, val_loss=4.5305\n",
      "train_acc=1.000, val_acc=0.747\n",
      "epoch=42\n",
      "train_loss=0.0019, val_loss=4.7852\n",
      "train_acc=1.000, val_acc=0.740\n",
      "epoch=43\n",
      "train_loss=0.0003, val_loss=5.2236\n",
      "train_acc=1.000, val_acc=0.744\n",
      "epoch=44\n",
      "train_loss=0.0002, val_loss=4.7146\n",
      "train_acc=1.000, val_acc=0.737\n",
      "epoch=45\n",
      "train_loss=0.0001, val_loss=4.9702\n",
      "train_acc=1.000, val_acc=0.758\n",
      "epoch=46\n",
      "train_loss=0.0000, val_loss=4.5568\n",
      "train_acc=1.000, val_acc=0.747\n",
      "epoch=47\n",
      "train_loss=0.0000, val_loss=4.5278\n",
      "train_acc=1.000, val_acc=0.753\n",
      "epoch=48\n",
      "train_loss=0.0000, val_loss=4.4738\n",
      "train_acc=1.000, val_acc=0.753\n",
      "epoch=49\n",
      "train_loss=0.0000, val_loss=4.4580\n",
      "train_acc=1.000, val_acc=0.749\n",
      "epoch=50\n",
      "train_loss=0.0000, val_loss=4.4221\n",
      "train_acc=1.000, val_acc=0.746\n",
      "epoch=51\n",
      "train_loss=0.0000, val_loss=4.4062\n",
      "train_acc=1.000, val_acc=0.747\n",
      "epoch=52\n",
      "train_loss=0.0001, val_loss=4.3867\n",
      "train_acc=1.000, val_acc=0.754\n",
      "epoch=53\n",
      "train_loss=0.0000, val_loss=4.3743\n",
      "train_acc=1.000, val_acc=0.754\n",
      "epoch=54\n",
      "train_loss=0.0000, val_loss=4.3887\n",
      "train_acc=1.000, val_acc=0.754\n",
      "epoch=55\n",
      "train_loss=0.0000, val_loss=4.4121\n",
      "train_acc=1.000, val_acc=0.753\n",
      "epoch=56\n",
      "train_loss=0.0000, val_loss=4.4267\n",
      "train_acc=1.000, val_acc=0.751\n",
      "epoch=57\n",
      "train_loss=0.0000, val_loss=4.4456\n",
      "train_acc=1.000, val_acc=0.751\n",
      "epoch=58\n",
      "train_loss=0.0000, val_loss=4.4583\n",
      "train_acc=1.000, val_acc=0.751\n",
      "epoch=59\n",
      "train_loss=0.0000, val_loss=4.4550\n",
      "train_acc=1.000, val_acc=0.749\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_char_x, test_char_x = pickle.load(open('char_final_train_x.pkl','rb')), pickle.load(open('char_final_test_x.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8544, 81, 300)\n"
     ]
    }
   ],
   "source": [
    "print(data_char_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_char_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
