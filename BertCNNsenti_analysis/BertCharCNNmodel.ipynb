{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from pytorch_transformers import BertTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    documents = documents.str.replace(\"[^a-zA-Z0-9 ]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence) # 토큰화\n",
    "        tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')] # 불용어 제거\n",
    "        clean_texts = \" \".join(tokens)\n",
    "        clean_texts = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', '', clean_texts) # 특수문자, 임티 제거\n",
    "        clean_texts = re.sub(' +', ' ', clean_texts) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen):\n",
    "    \"\"\"Right padding.\"\"\"\n",
    "    pad_seq = []\n",
    "    for sequence in sequences:\n",
    "        current_len = len(sequence)\n",
    "        if current_len > maxlen:\n",
    "            pad_seq.append(sequence[: maxlen - 1])\n",
    "        else:\n",
    "            extra = maxlen - current_len\n",
    "            pad_seq.append(sequence + ([0] * extra))\n",
    "\n",
    "    return pad_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(max_len, train_x, test_x):\n",
    "    \n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "\n",
    "    sequences = [tokenizer.encode(\"[CLS] \"+ t +\" [SEP]\", add_special_tokens=False) for t in squeezed_data] # 문자를 시퀀스로 바꿈\n",
    "    #sequences = tokenizer.batch_encode_plus(squeezed_data, add_special_tokens=True, return_attention_mask=True, padding='longest')\n",
    "    print('시퀀스 길이', len(sequences[0]))\n",
    "    print('첫번째 시퀀스 예시', sequences[0])\n",
    "\n",
    "    sequences_matrix = pad_sequences(sequences, max_len) # 길이를 max_len 으로 맞춤\n",
    "    #print('시퀀스 행렬',sequences_matrix)\n",
    "    print('첫번째 시퀀스 행렬 예시',sequences_matrix[0])\n",
    "    print('첫번째 시퀀스 행렬 길이',len(sequences_matrix[0]))\n",
    "    #print('총 단어 수 : ', len(tok.word_index))\n",
    "\n",
    "    train_result = sequences_matrix[:div]\n",
    "    test_result = sequences_matrix[div:]\n",
    "\n",
    "    return train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv(\"train_final.csv\")\n",
    "    eval = pd.read_csv(\"eval_final_open.csv\")\n",
    "    train_X = train['Sentence'] # 본문 내용을 기준으로 분류하기\n",
    "    train_Y = train['Category']\n",
    "    eval_X = eval['Sentence']\n",
    "    return train_X, train_Y, eval_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터셋 크기 :  15855\n",
      "시퀀스 길이 30\n",
      "첫번째 시퀀스 예시 [101, 1048, 15185, 1996, 2143, 25269, 2497, 10455, 8476, 6550, 19647, 14868, 7481, 5450, 2203, 5263, 2004, 17119, 18249, 3251, 2143, 4563, 6171, 21877, 18719, 23738, 2594, 5168, 17772, 102]\n",
      "첫번째 시퀀스 행렬 예시 [101, 1048, 15185, 1996, 2143, 25269, 2497, 10455, 8476, 6550, 19647, 14868, 7481, 5450, 2203, 5263, 2004, 17119, 18249, 3251, 2143, 4563, 6171, 21877, 18719, 23738, 2594, 5168, 17772, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "첫번째 시퀀스 행렬 길이 70\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "step1_flag = True\n",
    "if not step1_flag:\n",
    "    print(train_x[:10])\n",
    "\n",
    "    train_x, test_x = preprocessing(train_x), preprocessing(test_x)\n",
    "    pd.DataFrame(train_x).to_csv('temp_train_x.csv')\n",
    "    pd.DataFrame(train_y).to_csv('temp_train_y.csv')\n",
    "    pd.DataFrame(test_x).to_csv('temp_test_x.csv')\n",
    "    #print(train_x[:10])\n",
    "else:\n",
    "    # 임시 불러오기\n",
    "    train_x, test_x = pd.read_csv('temp_train_x.csv'), pd.read_csv('temp_test_x.csv')\n",
    "    #print(train_x)\n",
    "    train_x = train_x.iloc[:,1:].values.tolist()\n",
    "    test_x = test_x.iloc[:,1:].values.tolist()\n",
    "    #print(train_x)\n",
    "    #y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "    max_len = 70\n",
    "    #print(train_x[0])\n",
    "    x_train, x_test = Tokenization(max_len, train_x, test_x)\n",
    "    # print(x_train)\n",
    "    pd.DataFrame(x_train).to_csv('final_train_x.csv')\n",
    "    #pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "    pd.DataFrame(x_test).to_csv('final_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_char(documents):\n",
    "    documents = documents.str.replace(\"[ãàáâçèéíïñóöôûüæ\\t\\n\\s]\",\"\")\n",
    "    clean_documents = []\n",
    "    for sentence in documents:\n",
    "        sentence = sentence.lower()\n",
    "        clean_texts = re.sub(' ', '', sentence) # 다중 공백 제거\n",
    "        clean_documents.append(clean_texts)\n",
    "    return clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization_char(max_len, train_x, test_x, alphabet =\"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    \n",
    "    alpha_len = len(alphabet)\n",
    "    id_mat = np.identity(alpha_len)\n",
    "    div = len(train_x) # 분할 지점 저장\n",
    "    dataset = train_x + test_x # 학습, 테스트 데이터 합치기\n",
    "    #print('총 데이터셋 크기 : ', len(dataset))\n",
    "    squeezed_data = np.array(dataset).squeeze().tolist()\n",
    "    \n",
    "    max_len = 300\n",
    "    encode_data_set = np.zeros((len(dataset), alpha_len, max_len))\n",
    "    for idx, t in enumerate(squeezed_data):\n",
    "        char_seq = []\n",
    "        for i, c in enumerate(t):\n",
    "            if i >= max_len:\n",
    "                break\n",
    "            if alphabet.find(c) == -1:\n",
    "                print(c)\n",
    "                print(\"error\")\n",
    "                break\n",
    "            char_seq.append(id_mat[alphabet.find(c)].reshape(-1,1))\n",
    "        \n",
    "        char_len = len(char_seq)\n",
    "        if char_len < max_len:\n",
    "            char_seq.append(np.zeros((alpha_len, max_len - char_len)))\n",
    "        \n",
    "        encode_data_set[idx] = np.hstack(char_seq)\n",
    "        if idx == 0:\n",
    "            print(encode_data_set[idx])\n",
    "\n",
    "    \n",
    "\n",
    "    train_result = encode_data_set[:div]\n",
    "    test_result = encode_data_set[div:]\n",
    "\n",
    "    return train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"-lrb-thefilm-rrb-tacklesthetopicofrelationshipsinsuchastraightforward,emotionallyhonestmannerthatbytheend,it'simpossibletoascertainwhetherthefilmis,atitscore,deeplypessimisticorquietlyhopeful.\"]\n",
      "['lavishly,exhilaratinglytasteless.']\n",
      "['itisalsobeautifullyacted.']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x = load_data()\n",
    "step1_flag = True\n",
    "if not step1_flag:\n",
    "\n",
    "    train_x, test_x = preprocessing_char(train_x), preprocessing_char(test_x)\n",
    "    pd.DataFrame(train_x).to_csv('char_temp_train_x.csv')\n",
    "    pd.DataFrame(train_y).to_csv('char_temp_train_y.csv')\n",
    "    pd.DataFrame(test_x).to_csv('char_temp_test_x.csv')\n",
    "    #print(train_x[:10])\n",
    "else:\n",
    "    # 임시 불러오기\n",
    "    train_x, test_x = pd.read_csv('char_temp_train_x.csv'), pd.read_csv('char_temp_test_x.csv')\n",
    "    #print(train_x)\n",
    "    train_x = train_x.iloc[:,1:].values.tolist()\n",
    "    test_x = test_x.iloc[:,1:].values.tolist()\n",
    "    print(train_x[0])\n",
    "    print(train_x[1])\n",
    "    print(train_x[2])\n",
    "    #y_train, nb_classes = labeling(train_y)\n",
    "\n",
    "    max_len = 70\n",
    "    #print(train_x[0])\n",
    "    x_train, x_test = Tokenization_char(max_len, train_x, test_x)\n",
    "    print(x_train)\n",
    "    pickle.dump(x_train, open(\"./char_final_train_x.pkl\",\"wb\"))\n",
    "    #pd.DataFrame(x_train).to_csv('char_final_train_x.csv')\n",
    "    #pd.DataFrame(y_train).to_csv('final_train_y.csv')\n",
    "    pickle.dump(x_test, open(\"./char_final_test_x.pkl\",\"wb\"))\n",
    "    #pd.DataFrame(x_test).to_csv('char_final_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mxnet.gluon import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "from pytorch_transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, number_of_characters, max_len, dropout=0.5, number_of_classes=5, batch_size = 32):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "\n",
    "        # define conv layers\n",
    "\n",
    "        self.dropout_input = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(number_of_characters ,256,kernel_size=7, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU(), nn.MaxPool1d(3))\n",
    "\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "\n",
    "        input_shape = (batch_size, max_len, number_of_characters)\n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        # define linear layers\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.output_dimension, 1024), nn.ReLU(), nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Linear(1024, number_of_classes)\n",
    "\n",
    "        # initialize weights\n",
    "        self._create_weights()\n",
    "\n",
    "    # utility private functions\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        \n",
    "        return output_dimension\n",
    "\n",
    "    # forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        x = self.dropout_input(x)\n",
    "        #print(x.size())\n",
    "        #x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = F.softmax(x)\n",
    "        #print(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertCNNClassifier(nn.Module):\n",
    "    def __init__(self, bert, charcnn, num_classes=5, num_classifier=3, dropout=None):\n",
    "        super(MyBertCNNClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.charcnn =charcnn\n",
    "        self.num_classes = num_classes\n",
    "        self.num_classifier = num_classifier\n",
    "        \n",
    "        self.classifiers = [nn.Sequential(\n",
    "        nn.Linear(768 + 1024, 512),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(512, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(128, num_classes),\n",
    "        nn.Softmax()\n",
    "        ) for _ in range(self.num_classifier)]\n",
    "        \n",
    "        self.classifiers = torch.nn.ModuleList(self.classifiers)\n",
    "        self.classifiers_parameters = self.classifiers.parameters()\n",
    "\n",
    "    \n",
    "    def forward(self, x, _x):\n",
    "        bert_out = self.bert(x)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        #print(bert_out[1].size(), charcnn_out.size())\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        pred = [cls(cat_in) for cls in self.classifiers]\n",
    "        #pred = torch.mean(torch.cat(cls_pred, dim = 0), dim = 0)\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, x, _x):\n",
    "        bert_out = self.bert(x)\n",
    "        charcnn_out = self.charcnn(_x)\n",
    "        cat_in = torch.hstack([bert_out[1], charcnn_out])\n",
    "        #classifers = [self.classifier(bert_out[1]).view(1,-1,self.num_classes) for _ in range(self.num_classifier)]\n",
    "        cls_pred = torch.hstack([torch.argmax(cls(cat_in), dim = 1).view(-1,1) for cls in self.classifiers])\n",
    "        pred, _ = torch.mode(cls_pred)\n",
    "        \n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, N, lossfn, optimizer, X, _X, Y, batch_size=16):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    #print(Y)\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_train_x, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "            #batch_train_x, batch_train_y = torch.tensor(X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_train_x, batch_train_char_x, batch_train_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_train_x, batch_train_char_x)\n",
    "        #print(logits)\n",
    "        for idx, logit in enumerate(logits):\n",
    "            if idx == 0:\n",
    "                loss = lossfn(logit, batch_train_y)\n",
    "            else:\n",
    "                loss += lossfn(logit, batch_train_y)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        model.eval()\n",
    "        pred_labels = model.predict(batch_train_x, batch_train_char_x)\n",
    "        #print(pred_labels)\n",
    "        train_acc += (pred_labels == batch_train_y).sum().item()\n",
    "        \n",
    "        del batch_train_x\n",
    "        del batch_train_char_x\n",
    "        del batch_train_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    train_loss /= N\n",
    "    train_acc /= N\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, N, lossfn, X, _X,  Y, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    loss, acc = 0.0, 0.0\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_valid_x, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device), torch.tensor(Y[batch_size * nb:]).long().to(device)\n",
    "        else:\n",
    "            batch_valid_x, batch_valid_char_x, batch_valid_y = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device), torch.tensor(Y[batch_size * i:batch_size * (i+1)]).long().to(device)\n",
    "        \n",
    "        logits = model(batch_valid_x, batch_valid_char_x)\n",
    "        for logit in logits:\n",
    "            loss += lossfn(logit, batch_valid_y).item() \n",
    "            \n",
    "        pred_labels = model.predict(batch_valid_x, batch_valid_char_x)\n",
    "        acc += (pred_labels == batch_valid_y).sum().item()\n",
    "            \n",
    "        del batch_valid_x\n",
    "        del batch_valid_char_x\n",
    "        del batch_valid_y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    loss /= N\n",
    "    acc /= N\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, N, lossfn, X, _X, batch_size=8):\n",
    "    #generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    test_y = None\n",
    "    nb = N // batch_size\n",
    "    for i in range(nb + 1):\n",
    "        if i == nb:\n",
    "            batch_test_x, batch_test_char_x = torch.tensor(X[batch_size * nb:]).long().to(device), torch.tensor(_X[batch_size * nb:]).float().to(device)\n",
    "        else:\n",
    "            batch_test_x, batch_test_char_x = torch.tensor(X[batch_size * i:batch_size * (i+1)]).long().to(device), torch.tensor(_X[batch_size * i:batch_size * (i+1)]).float().to(device)\n",
    "\n",
    "        #logits = model(batch_test_x)\n",
    "        batch_test_y = model.predict(batch_test_x, batch_test_char_x).detach().cpu().numpy()\n",
    "        if i == 0:\n",
    "            test_y = batch_test_y\n",
    "        else:\n",
    "            test_y = np.concatenate([test_y, batch_test_y])\n",
    "            \n",
    "        del batch_test_x\n",
    "        del batch_test_char_x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(binary=False, bert=\"bert-base-uncased\", epochs=30, batch_size=16, train_prop = 0.9, alphabet = \"\"\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\"\\/|_@#$%^&*~`+-=<>()[]{}\"\"\"):\n",
    "    \n",
    "    data_x, test_x, data_y = np.array(pd.read_csv('final_train_x.csv').iloc[:,1:]), np.array(pd.read_csv('final_test_x.csv').iloc[:,1:]), np.array(pd.read_csv('temp_train_y.csv').iloc[:,1:])\n",
    "    data_char_x, test_char_x  = pickle.load(open('char_final_train_x.pkl','rb')), pickle.load(open('char_final_test_x.pkl','rb'))\n",
    "    data_len = data_x.shape[0]\n",
    "    test_len = test_x.shape[0]\n",
    "    train_div = np.int32(data_len * train_prop)\n",
    "    comp_data = list(zip(data_x, data_char_x, data_y))\n",
    "    np.random.shuffle(comp_data)\n",
    "    data_x, data_char_x, data_y = zip(*comp_data)\n",
    "    data_x, data_char_x, data_y = np.array(data_x), np.array(data_char_x), np.array(data_y)\n",
    "    train_x, train_char_x, train_y = data_x[:train_div], data_char_x[:train_div], data_y[:train_div].reshape(-1)\n",
    "    valid_x, valid_char_x, valid_y = data_x[train_div:], data_char_x[train_div:], data_y[train_div:].reshape(-1)\n",
    "\n",
    "\n",
    "    config = BertConfig.from_pretrained(bert)\n",
    "    if not binary:\n",
    "        config.num_labels = 5\n",
    "    bert_model = BertModel.from_pretrained(bert, config=config)\n",
    "    charcnn_model = CharacterLevelCNN(number_of_characters = len(alphabet), max_len = 300)\n",
    "    #charcnn_model.load_state_dict(copy.deepcopy(torch.load(\"char_cnn_pre.pth\", torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))))\n",
    "    charcnn_mode = torch.load(\"char_cnn_pre.pth\", torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    model = MyBertCNNClassifier(bert_model, charcnn_model, num_classes=5, dropout=0.3)\n",
    "\n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_div, lossfn, optimizer, train_x ,train_char_x, train_y, batch_size=batch_size)\n",
    "        val_loss, val_acc = evaluate_one_epoch(model, data_len - train_div, lossfn,  valid_x, valid_char_x, valid_y, batch_size=batch_size)\n",
    "        scheduler.step(val_loss)\n",
    "        test_out = test_one_epoch(model, test_len, lossfn, test_x, test_char_x, batch_size=batch_size)\n",
    "        sub_data = pd.DataFrame(test_out.reshape((-1,1)), columns=['Category'])\n",
    "        #test_loss, test_acc = evaluate_one_epoch(model, lossfn, optimizer, testset, batch_size=batch_size)\n",
    "\n",
    "        print(f\"epoch={epoch}\")\n",
    "        print(f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        print(f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "        sub_data.to_csv('submission'+str(epoch)+'.csv', index_label = ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'src.cnn_model.CharacterLevelCNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1\n",
      "train_loss=0.2961, val_loss=0.2946\n",
      "train_acc=0.285, val_acc=0.337\n",
      "epoch=2\n",
      "train_loss=0.2798, val_loss=0.2813\n",
      "train_acc=0.434, val_acc=0.412\n",
      "epoch=3\n",
      "train_loss=0.2671, val_loss=0.2778\n",
      "train_acc=0.500, val_acc=0.434\n",
      "epoch=4\n",
      "train_loss=0.2605, val_loss=0.2742\n",
      "train_acc=0.537, val_acc=0.450\n",
      "epoch=5\n",
      "train_loss=0.2564, val_loss=0.2801\n",
      "train_acc=0.555, val_acc=0.421\n",
      "epoch=6\n",
      "train_loss=0.2533, val_loss=0.2768\n",
      "train_acc=0.569, val_acc=0.435\n",
      "epoch=7\n",
      "train_loss=0.2513, val_loss=0.2687\n",
      "train_acc=0.577, val_acc=0.484\n",
      "epoch=8\n",
      "train_loss=0.2490, val_loss=0.2685\n",
      "train_acc=0.590, val_acc=0.486\n",
      "epoch=9\n",
      "train_loss=0.2479, val_loss=0.2679\n",
      "train_acc=0.595, val_acc=0.487\n",
      "epoch=10\n",
      "train_loss=0.2466, val_loss=0.2760\n",
      "train_acc=0.599, val_acc=0.448\n",
      "epoch=11\n",
      "train_loss=0.2453, val_loss=0.2733\n",
      "train_acc=0.607, val_acc=0.457\n",
      "epoch=12\n",
      "train_loss=0.2446, val_loss=0.2689\n",
      "train_acc=0.610, val_acc=0.482\n",
      "epoch=13\n",
      "train_loss=0.2429, val_loss=0.2708\n",
      "train_acc=0.618, val_acc=0.471\n",
      "epoch=14\n",
      "train_loss=0.2415, val_loss=0.2690\n",
      "train_acc=0.623, val_acc=0.480\n",
      "epoch=15\n",
      "train_loss=0.2399, val_loss=0.2677\n",
      "train_acc=0.628, val_acc=0.483\n",
      "epoch=16\n",
      "train_loss=0.2379, val_loss=0.2680\n",
      "train_acc=0.643, val_acc=0.483\n",
      "epoch=17\n",
      "train_loss=0.2362, val_loss=0.2661\n",
      "train_acc=0.662, val_acc=0.497\n",
      "epoch=18\n",
      "train_loss=0.2353, val_loss=0.2648\n",
      "train_acc=0.665, val_acc=0.502\n",
      "epoch=19\n",
      "train_loss=0.2333, val_loss=0.2625\n",
      "train_acc=0.673, val_acc=0.517\n",
      "epoch=20\n",
      "train_loss=0.2328, val_loss=0.2644\n",
      "train_acc=0.678, val_acc=0.504\n",
      "epoch=21\n",
      "train_loss=0.2289, val_loss=0.2608\n",
      "train_acc=0.697, val_acc=0.525\n",
      "epoch=22\n",
      "train_loss=0.2262, val_loss=0.2572\n",
      "train_acc=0.707, val_acc=0.547\n",
      "epoch=23\n",
      "train_loss=0.2244, val_loss=0.2568\n",
      "train_acc=0.718, val_acc=0.548\n",
      "epoch=24\n",
      "train_loss=0.2227, val_loss=0.2576\n",
      "train_acc=0.727, val_acc=0.543\n",
      "epoch=25\n",
      "train_loss=0.2207, val_loss=0.2575\n",
      "train_acc=0.735, val_acc=0.545\n",
      "epoch=26\n",
      "train_loss=0.2195, val_loss=0.2566\n",
      "train_acc=0.742, val_acc=0.547\n",
      "epoch=27\n",
      "train_loss=0.2184, val_loss=0.2545\n",
      "train_acc=0.748, val_acc=0.561\n",
      "epoch=28\n",
      "train_loss=0.2176, val_loss=0.2555\n",
      "train_acc=0.754, val_acc=0.554\n",
      "epoch=29\n",
      "train_loss=0.2171, val_loss=0.2548\n",
      "train_acc=0.756, val_acc=0.558\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
