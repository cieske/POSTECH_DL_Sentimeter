{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooperative-motor",
   "metadata": {},
   "source": [
    "## Pre-training custom classifier using SST-5\n",
    "\n",
    "Reference: [ShannonAI Github](https://github.com/ShannonAI/Self_Explaining_Structures_Improve_NLP_Models)  \n",
    "\n",
    "What used?\n",
    "  * BertTokenizer\n",
    "  * BertForSequenceClassification\n",
    " \n",
    "You can download SST-5 dataset from above reference. Please check \"Prepare Datasets and Models\" section.\n",
    "\n",
    "Dataset consists of 3 files, \"train.txt, dev.txt, test.txt\". I use train.txt and dev.txt as training set, and test.txt as validation set.\n",
    "\n",
    "Custom classifier is MLP with 1 hidden layer. In \"Save model parameters\" section, entire model parameters are saved. If you want to use this pre-trained result with different custom layer, yo have to modify your classifier in this file and \"Bert-custom_linear.ipynb\" simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-kuwait",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T11:40:51.945899Z",
     "start_time": "2021-05-18T11:40:51.930966Z"
    }
   },
   "source": [
    "### Requirement\n",
    "  * transformers == 4.6.0\n",
    "  * pytorch == 1.8.0\n",
    "  * numpy == 1.1.92\n",
    "  * pandas == 1.2.3\n",
    "  * tqdm == 4.60.0\n",
    "  * scikit-learn == 0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-upset",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T11:44:14.460119Z",
     "start_time": "2021-05-18T11:44:14.451143Z"
    }
   },
   "source": [
    "<a id=\"Table\"></a>\n",
    "### Table  \n",
    "  - [Import library](#Import)\n",
    "  - [Check train set](#Trainset)\n",
    "  - [Tokenizer, Encoding data](#Encode)\n",
    "  - [Define custom classifier model](#Classifier)\n",
    "  - [Call custom classifier model, Data loaders](#Model)\n",
    "    - Can modify batch_size in this block\n",
    "  - [Optimizer & Scheduler](#Optim)\n",
    "    - Can modify epochs, lr, etc. in this block\n",
    "  - [Running model](#Run)\n",
    "  - [Save model parameters](#Save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-discretion",
   "metadata": {},
   "source": [
    "<a id=\"Import\"></a>\n",
    "### Import library\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defensive-alignment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:15:20.089308Z",
     "start_time": "2021-05-26T17:15:18.780034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertForMaskedLM, BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hourly-fifteen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:15:20.134421Z",
     "start_time": "2021-05-26T17:15:20.090308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "print('Use', 'cuda' if USE_CUDA else 'cpu')\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-moses",
   "metadata": {},
   "source": [
    "<a id=\"Trainset\"></a>\n",
    "### Check train set\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "discrete-laundry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:19:38.397663Z",
     "start_time": "2021-05-26T17:19:38.363518Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./SST_modified/train.txt\", sep=\"\\t\", names=['label', 'Sentence'], header=None)\n",
    "dev = pd.read_csv(\"./SST_modified/dev.txt\", sep=\"\\t\", names=['label', 'Sentence'], header=None)\n",
    "test = pd.read_csv(\"./SST_modified/test.txt\", sep=\"\\t\", names=['label', 'Sentence'], header=None)\n",
    "#train_df = pd.read_csv('./data/aug_train_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stylish-might",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:19:39.276856Z",
     "start_time": "2021-05-26T17:19:39.267879Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, dev],  ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-auction",
   "metadata": {},
   "source": [
    "Label distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coordinate-creativity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:19:41.823130Z",
     "start_time": "2021-05-26T17:19:41.806790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2601\n",
       "1    2507\n",
       "2    1853\n",
       "4    1453\n",
       "0    1231\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-candle",
   "metadata": {},
   "source": [
    "<a id=\"Encode\"></a>\n",
    "### Tokenizer, Encoding data\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "funky-plenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:19:56.149921Z",
     "start_time": "2021-05-26T17:19:49.020736Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use below tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "#Encode train sentence\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train_df.Sentence.values, #Sentence data\n",
    "    add_special_tokens=True,    #Encoded with special tokens relative to their model\n",
    "    return_attention_mask=True, #Return attention mask according to tokenizer defined by max_length att.\n",
    "    padding='longest',           #Padding!\n",
    "    #truncation=True,            \n",
    "    return_tensors='pt'         #Return torch tensor\n",
    ")\n",
    "\n",
    "#Encode validation sentence\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    test.Sentence.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='longest', \n",
    "    #truncation=True, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "typical-peoples",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:20:16.570236Z",
     "start_time": "2021-05-26T17:20:16.554615Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "encoded_data_train has 3 keys:\n",
    "  *input_ids\n",
    "  *token_type_ids\n",
    "  *attention_mask\n",
    "\"\"\"\n",
    "\n",
    "#Input\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(train_df.label.values)\n",
    "\n",
    "#Validation\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(test.label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-alpha",
   "metadata": {},
   "source": [
    "<a id=\"Classifier\"></a>\n",
    "### Define custom classifier model\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepting-crazy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:20:41.214927Z",
     "start_time": "2021-05-26T17:20:41.198942Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size//2)\n",
    "        self.output = nn.Linear(self.bert.config.hidden_size//2,  n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask)\n",
    "        x = self.drop1(x[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-saying",
   "metadata": {},
   "source": [
    "<a id=\"Model\"></a>\n",
    "### Call custom classifier model, Data loaders\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beautiful-bridal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:20:51.293117Z",
     "start_time": "2021-05-26T17:20:47.825353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop1): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (output): Linear(in_features=384, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(len(train_df.label.unique()))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "similar-consistency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:20:54.404945Z",
     "start_time": "2021-05-26T17:20:54.400955Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-finger",
   "metadata": {},
   "source": [
    "<a id=\"Optim\"></a>\n",
    "### Optimizer & Scheduler\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "analyzed-bradley",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:21:04.058251Z",
     "start_time": "2021-05-26T17:21:04.047246Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-playlist",
   "metadata": {},
   "source": [
    "<a id=\"Run\"></a>\n",
    "### Running model\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "advance-block",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:20:59.999079Z",
     "start_time": "2021-05-26T17:20:59.914094Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    losses = []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        predictions.append(preds)\n",
    "        \n",
    "        loss = loss_fn(outputs, batch[2])\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == batch[2])\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return correct_predictions.double() / len(dataloader_validation.dataset), np.mean(losses), predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pursuant-reconstruction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:26:48.225930Z",
     "start_time": "2021-05-26T17:21:30.621459Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbef1f89921441bb0d282b4b43335a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.4727733006722497\n",
      "Validation loss: 1.404926652530972\n",
      "Valdation accuracy: 0.4995475113122172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 1.3756605463834544\n",
      "Validation loss: 1.4008609107930026\n",
      "Valdation accuracy: 0.497737556561086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 1.3212854397632985\n",
      "Validation loss: 1.3861511791352745\n",
      "Valdation accuracy: 0.5190045248868779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 1.2715625142181295\n",
      "Validation loss: 1.3901194325453943\n",
      "Valdation accuracy: 0.5108597285067874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 1.2377172241954266\n",
      "Validation loss: 1.3869493513656177\n",
      "Valdation accuracy: 0.5113122171945702\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, batch[2])\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == batch[2])\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "    #If you want to save model parameters, please modify below code.\n",
    "    #Line 80 save model for every epochs\n",
    "    #torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = np.mean(losses)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_acc, val_loss, _ = evaluate(dataloader_validation)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'Valdation accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-consumption",
   "metadata": {},
   "source": [
    "<a id=\"Save\"></a>\n",
    "### Save model parameters\n",
    "  - [Return to table](#Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "classified-malta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T17:28:08.715469Z",
     "start_time": "2021-05-26T17:28:08.162398Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_save/SST_5_fine_tuning.model')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
